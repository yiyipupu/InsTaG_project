# 实时对话模块

​	本模块实现了基于Client-Server 架构的低延迟实时语音对话系统，实现了“语音输入 -> 文本识别 -> 大脑思考 -> 语音合成 -> 驱动播放”的过程。

## 一、配置与使用

客户端代码运行在本地Windows/Linux环境中，负责音频采集和逻辑调度。

### 1. 依赖安装

请确保本地安装了 Python 3.8+，并安装以下依赖库：

Bash

```
pip install requests sounddevice soundfile numpy openai
```

### 2. 配置文件 (`main.py` & `llm_client.py`)

**修改 `main.py` 中的服务器配置：**

Python

```
# 填入你的华为云公网 IP
IP_ADDRESS = "124.70.54.5" 
# 确保端口与云端 Docker 映射一致
ASR_URL = f"http://{IP_ADDRESS}:8001/asr"
TTS_URL = f"http://{IP_ADDRESS}:8000/inference/zero-shot"
# 选择正确的麦克风设备 ID (使用 sd.query_devices() 查看)
MIC_DEVICE_INDEX = 1 
```

**修改 `llm_client.py` 中的 API Key：**

Python

```
# 申请 DeepSeek API Key: https://platform.deepseek.com/
API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxx" 
```

### 3. 运行方法

1. 确保目录下存在 `prompt.wav`（用于克隆声音的参考音频）。

2. 修改`main.py`中的 PROMPT_TEXT，PROMPT_TEXT为prompt音频所说的文本，越精准越好，而且要加上标点符号，用于对齐；如果不修改PROMPT_TEXT会造成胡言乱语的现象。

   ```python
   PROMPT_WAV = "prompt.wav"
   PROMPT_TEXT = "你好，我是语音助手，很高兴为你服务。"
   ```

   

3. 运行主程序：

   Bash

   ```
   python main.py
   ```

4. 按提示操作：

   - 按 回车开始录音。
   - 说完话后再次按 回车结束录音。
   - 系统将自动开始识别、思考并进行语音回复。



## 二、系统架构与原理

本系统将高算力的 AI 推理任务部署在云端，轻量级的控制逻辑运行在本地客户端。

### 数据流转流程

1. **ASR**: 客户端采集麦克风音频，发送至云端SenseVoice服务进行识别。
2. **LLM**: 识别后的文本发送至DeepSeek大模型，通过流式接口 (Stream) 获取实时回复。
3. **Buffering**: 客户端利用智能分句算法，将LLM的流式输出切分为完整的短句。
4. **TTS**: 短句并行发送至云端 CosyVoice服务，合成高质量语音。
5. **Player**: 客户端维护音频队列，按顺序播放合成的语音（未来此处对接 InsTaG 视频生成接口）。



## 三、服务端部署 (华为云)

服务端运行在 华为云 ECS (Tesla T4) 服务器上，采用 Docker 技术实现 AI 服务的隔离部署。

### 1. 环境准备

- **硬件**: 华为云 ECS AI 加速型 (Nvidia Tesla T4, 16GB 显存)。
- **系统**: Ubuntu 22.04。
- **基础依赖**: 安装 Nvidia Drivers, CUDA Toolkit, Docker, Nvidia-Container-Toolkit。

### 2. 隔离部署方案

为了解决不同模型间的依赖冲突（如 Python 版本、CUDA 版本不一致），我们为 ASR 和 TTS 构建了独立的容器：

- **ASR 容器 (SenseVoice)**
  - 映射端口: `8001`
  - 功能: 接收 WAV 音频，返回文字。
- **TTS 容器 (CosyVoice)**
  - 映射端口: `8000`
  - 功能: 接收文本与参考音频 (Prompt)，返回合成音频。

### 3. 启动服务 (示例指令)

Bash

```
# 启动 TTS 服务
docker run -d --gpus all -p 8000:8000 --name cosyvoice_api cosyvoice:v1

# 启动 ASR 服务
docker run -d --gpus all -p 8001:8001 --name sensevoice_api sensevoice:v1
```



## 四、代码实现细节

### 1. 流式分句处理

为了减少 TTS 的等待时间，我们实现了 `split_text_stream` 生成器。它不等待 LLM 生成完整的回复，而是利用标点符号（如 `。！？`）将流式文本实时切分为短句。

首字响应时间 大幅降低，用户体验更流畅。

### 2. 生产者-消费者模型

`main.py` 使用了多线程和队列来实现并行处理：

- **主线程**: 负责录音、ASR 请求、LLM 流式接收与分句。
- **TTS 线程 (`tts_worker`)**: 从 `text_queue` 获取短句，发送 HTTP 请求给云端，合成音频后放入 `audio_queue`。
- **播放线程 (`play_worker`)**: 从 `audio_queue` 获取音频文件并播放。

### 3. LLM 客户端封装

`llm_client.py` 封装了 DeepSeek 的 API 调用，并包含了一个关键的清洗函数 `clean_sensevoice_output`，用于去除 ASR 模型可能产生的特殊标签（如 `<|zh|><|NEUTRAL|>`），确保发送给 LLM 的是纯净文本。

### 4. 声音克隆 (Zero-Shot TTS)

系统利用 CosyVoice 的 Zero-Shot 能力。每次 TTS 请求都会携带本地的 `prompt.wav` 和 `PROMPT_TEXT`，云端模型会即时分析该音频的音色，并用相同的音色合成回复语音。